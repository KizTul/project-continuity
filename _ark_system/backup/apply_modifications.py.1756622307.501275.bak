import argparse
import json
import os
import sys
import logging
import jsonschema
from datetime import datetime
import hashlib
from tempfile import NamedTemporaryFile
import time
import shutil

# --- New Import --- 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from _ark_system._tools.ark_checksum import calculate_clean_checksum, extract_checksum_tag, BOM_BYTES

# --- Constants ---
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
MOD_PACKAGE_PATH = os.path.join(ROOT_DIR, '_ark_system', '_staging', 'modification_package.json')
LOG_DIR = os.path.join(ROOT_DIR, '_ark_system', 'logs')
RECEIPT_DIR = os.path.join(LOG_DIR, 'receipts')
BACKUP_DIR = os.path.join(ROOT_DIR, '_ark_system', 'backup')
LOCK_FILE_PATH = os.path.join(ROOT_DIR, '_ark_system', '_staging', '.apply.lock')

MODIFICATION_SCHEMA_V1_1 = {
    "type": "object",
    "properties": {
        "version": {"type": "string", "pattern": "^1\\.1$"},
        "modifications": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "action": {"type": "string"},
                    "path": {"type": "string"},
                    "content": {"type": "string"},
                    "expected_checksum_before": {"type": ["string", "null"]}
                },
                "required": ["action", "path"]
            }
        }
    },
    "required": ["version", "modifications"]
}

CHECKSUM_TAG_FORMAT = "<!-- [ARK_INTEGRITY_CHECKSUM::sha256:{}] -->"

def setup_logging(log_path):
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger()

class Transaction:
    def __init__(self, operations, logger, dry_run=False):
        self.operations = operations
        self.logger = logger
        self.dry_run = dry_run
        self.undo_log = []
        self.receipt = {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "PENDING",
            "dry_run": self.dry_run,
            "operation_count": len(self.operations),
            "actions": [],
            "updated_files": [],
            "rollback_errors": []
        }

    def execute(self):
        self.logger.info(f"--- [ARK Modification Processor v7.0 - Golden Standard] ---")
        try:
            jsonschema.validate(instance={"version": "1.1", "modifications": self.operations}, schema=MODIFICATION_SCHEMA_V1_1)
            self.logger.info("VALIDATION SUCCESS: Package conforms to schema.")
        except jsonschema.exceptions.ValidationError as e:
            self.logger.error(f"VALIDATION FAILED: {e.message}")
            self.receipt["status"] = "FAIL"
            return

        for i, op in enumerate(self.operations):
            try:
                self.process_operation(i + 1, op)
            except Exception as e:
                self.logger.error(f"FATAL ERROR during execution: {e}")
                self.logger.warning("--- [Initiating ROLLBACK] ---")
                self.rollback()
                self.logger.warning("--- [ROLLBACK Complete] ---")
                self.receipt["status"] = "FAIL"
                return

        self.receipt["status"] = "SUCCESS"

    def rollback(self):
        for undo_op in reversed(self.undo_log):
            try:
                if undo_op['action'] == 'RESTORE_FILE':
                    shutil.copy2(undo_op['backup_path'], undo_op['original_path'])
                    self.logger.info(f"ROLLED BACK: Restored '{undo_op['original_path']}'")
                elif undo_op['action'] == 'DELETE_FILE':
                    os.remove(undo_op['path'])
                    self.logger.info(f"ROLLED BACK: Deleted '{undo_op['path']}'")
            except Exception as e:
                self.logger.error(f"ROLLBACK FAILED for {undo_op}: {e}")
                self.receipt['rollback_errors'].append(str(e))

    def process_operation(self, i, op):
        action = op.get('action')
        path = op.get('path')
        full_path = os.path.join(ROOT_DIR, path)
        self.logger.info(f"--- Step {i}/{len(self.operations)}: [{action}] on [{path}] ---")

        original_bytes = self._read_file_with_retry(full_path, self.logger)
        actual_checksum = calculate_clean_checksum(original_bytes)
        expected_checksum = op.get('expected_checksum_before')

        if expected_checksum is not None and actual_checksum != expected_checksum:
            raise Exception(f"STATE CONFLICT on '{path}'. Expected '{expected_checksum}', found '{actual_checksum}'.")

        if action == 'MODIFY_FILE':
            backup_path = os.path.join(BACKUP_DIR, f"{os.path.basename(path)}.{datetime.utcnow().timestamp()}.bak")
            if os.path.exists(full_path):
                shutil.copy2(full_path, backup_path)
                self.undo_log.append({'action': 'RESTORE_FILE', 'original_path': full_path, 'backup_path': backup_path})
            else:
                self.undo_log.append({'action': 'DELETE_FILE', 'path': full_path})
            
            self.logger.info(f"BACKUP: Saved '{path}' to backup.")

            content = op.get('content', '')
            final_content_bytes = content.encode('utf-8')
            final_clean_checksum = calculate_clean_checksum(final_content_bytes)
            is_verifiable = path.endswith('.md')

            if not self.dry_run:
                with NamedTemporaryFile('wb', delete=False, dir=os.path.dirname(full_path), suffix='.tmp') as tf:
                    tf.write(final_content_bytes)
                    if is_verifiable:
                        tag = f"\n\n{CHECKSUM_TAG_FORMAT.format(final_clean_checksum)}"
                        tf.write(tag.encode('utf-8'))
                    tmp_name = tf.name
                os.replace(tmp_name, full_path)
            
            file_after_bytes = self._read_file_with_retry(full_path, self.logger)
            verified_clean_checksum = calculate_clean_checksum(file_after_bytes)

            if final_clean_checksum != verified_clean_checksum:
                raise Exception("POST-WRITE VERIFICATION FAILED!")

            self.receipt['updated_files'].append({"path": path, "new_checksum": verified_clean_checksum})
            self.logger.info(f"SUCCESS: File '{path}' processed and verified.")

        elif action == 'CREATE_FILE':
            self.undo_log.append({'action': 'DELETE_FILE', 'path': full_path})
            content = op.get('content', '')
            final_content_bytes = content.encode('utf-8')
            final_clean_checksum = calculate_clean_checksum(final_content_bytes)
            is_verifiable = path.endswith('.md')

            if not self.dry_run:
                with NamedTemporaryFile('wb', delete=False, dir=os.path.dirname(full_path), suffix='.tmp') as tf:
                    tf.write(final_content_bytes)
                    if is_verifiable:
                        tag = f"\n\n{CHECKSUM_TAG_FORMAT.format(final_clean_checksum)}"
                        tf.write(tag.encode('utf-8'))
                    tmp_name = tf.name
                os.replace(tmp_name, full_path)

            self.receipt['updated_files'].append({"path": path, "new_checksum": final_clean_checksum})
            self.logger.info(f"SUCCESS: File '{path}' created.")

    def _read_file_with_retry(self, path, logger):
        if not os.path.exists(path):
            return None
        try:
            with open(path, 'rb') as f:
                return f.read()
        except Exception as e:
            logger.warning(f"Could not read file {path}: {e}")
            return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dry-run', action='store_true')
    args = parser.parse_args()

    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(RECEIPT_DIR, exist_ok=True)
    os.makedirs(BACKUP_DIR, exist_ok=True)

    log_ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
    log_path = os.path.join(LOG_DIR, f'apply_changes_{log_ts}.log')
    logger = setup_logging(log_path)

    try:
        with open(LOCK_FILE_PATH, 'x') as f:
            f.write(str(os.getpid()))
        logger.info(f"Lock acquired by PID {os.getpid()}.")
    except FileExistsError:
        logger.error("Lock file exists. Another instance may be running.")
        sys.exit(1)

    transaction = None
    try:
        with open(MOD_PACKAGE_PATH, 'r', encoding='utf-8') as f:
            package_data = json.load(f)
        
        transaction = Transaction(package_data.get('modifications', []), logger, args.dry_run)
        transaction.execute()

    except Exception as e:
        logger.critical(f"An unhandled exception occurred: {e}")
        if transaction:
            transaction.receipt["status"] = "FAIL"
    finally:
        if transaction:
            receipt_ts = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            receipt_path = os.path.join(RECEIPT_DIR, f'receipt_{receipt_ts}.json')
            with open(receipt_path, 'w', encoding='utf-8') as f:
                json.dump(transaction.receipt, f, indent=2)
            logger.info(f"Receipt saved to {receipt_path}")
            print("--- [Transaction Complete] ---")
            print(f"SUCCESS: {len(transaction.receipt['updated_files'])} operations applied." if transaction.receipt['status'] == 'SUCCESS' else "FAILURE: Transaction failed and was rolled back.")

        if os.path.exists(LOCK_FILE_PATH):
            os.remove(LOCK_FILE_PATH)
            logger.info("Lock file released.")
        
        # Cleanup old backups
        backup_files = [os.path.join(BACKUP_DIR, f) for f in os.listdir(BACKUP_DIR)]
        if backup_files:
            backup_files.sort(key=os.path.getmtime)
            # Keep the most recent, for example, 20 backups
            while len(backup_files) > 20:
                os.remove(backup_files.pop(0))
            logger.info("Backup directory cleaned up.")

if __name__ == '__main__':
    main()
